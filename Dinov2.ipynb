{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samilarinc/engineerdocs/blob/main/Dinov2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install -q timm\n",
        "\n",
        "#vit_base_patch16_224:    85m parameters (1,768) output\n",
        "#vit_small_resnet26d_224: 63m parameters (1,768) output\n",
        "#vit_small_patch16_224:   22m parameters (1,384) output\n",
        "#vit_tiny_patch16_224:    5m  parameters (1,192) output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ow-MZio7nAon",
        "outputId": "245886bb-d864-4663-9972-76ddca872a50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm *.csv\n",
        "if \"csv_folder\" not in os.listdir(\"/content/\"):\n",
        "    os.system('cp drive/MyDrive/csv_folder.zip .')\n",
        "    os.system('unzip csv_folder.zip -d .')\n",
        "\n",
        "if \"processed_teknofest\" not in os.listdir(\"/content/\"):\n",
        "    os.system('cp /content/drive/MyDrive/processed_teknofest.zip .')\n",
        "    os.system('unzip processed_teknofest.zip -d .')\n",
        "\n",
        "task = 0 # int(input('0 for birads, 1 for kompo: '))\n",
        "batch_size = 16 # int(input('Enter batch size: '))\n",
        "target_names = [['BI-RADS0', 'BI-RADS1-2','BI-RADS4-5'], ['A', 'B', 'C', 'D']][task]\n",
        "\n",
        "data_list = [\"teknofest\"]\n",
        "if False: #input(\"Use VinDr (n/y): \") == \"y\":\n",
        "  data_list.append(\"vindr\")\n",
        "  os.system('cp /content/drive/MyDrive/processed_vindr.zip .')\n",
        "  os.system('unzip processed_vindr.zip -d .')\n",
        "  os.system('rm processed_vindr/dbca9d28baa3207b3187c4d07dc81a80*')\n",
        "\n",
        "if input(\"Use rsna0 (n/y): \") == \"y\":\n",
        "  data_list.append(\"rsna0\")\n",
        "  os.system('cp /content/drive/MyDrive/rsna_birads0_clahe.zip .')\n",
        "  os.system('unzip rsna_birads0_clahe.zip')\n",
        "  os.system('mv rsna_birads0_clahe processed_rsna0')\n",
        "  os.system('cp drive/MyDrive/rsna0_birads_fixed.csv .')\n",
        "\n",
        "model_name = \"vit_tiny_patch16_224\" # input(\"Enter model name (timm or torch): \") #vit_tiny_patch16_224"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5G8C_4x_nGDd",
        "outputId": "b0bed512-daae-4e27-c551-e47e0c39c7a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Use rsna0 (n/y): n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import gc\n",
        "import cv2\n",
        "import time\n",
        "import timm\n",
        "\n",
        "from tqdm import tqdm\n",
        "from time import perf_counter\n",
        "from torchsummary import summary\n",
        "\n",
        "import torch\n",
        "t = torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
        "import torchvision as tv\n",
        "from torchvision import io\n",
        "import torchvision.transforms as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torchsummary import summary\n",
        "\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import warnings\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "device = 'cuda' if t.cuda.is_available() else 'cpu'\n",
        "# !nvidia-smi"
      ],
      "metadata": {
        "id": "MnoUdNYNnxA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MemeDataset(Dataset):\n",
        "    def __init__(self, image_root_dir, transform, csv_file, model_type = 'MEME KOMPOZİSYONU'):\n",
        "        self.data = pd.read_csv(csv_file, index_col = 0)\n",
        "        y = pd.get_dummies(self.data[model_type], prefix = '')\n",
        "        if model_type != 'MEME KOMPOZİSYONU':\n",
        "            if image_root_dir.split('_')[1] == 'rsna':\n",
        "                self.data['one_hot'] = list(np.hstack((np.array(y), np.zeros((y.shape[0], 1)))))\n",
        "\n",
        "            if image_root_dir.split('_')[1] == 'rsna0':\n",
        "                self.data['one_hot'] = list(np.hstack((np.array(y), np.zeros((y.shape[0], 1)))))\n",
        "\n",
        "            elif image_root_dir.split('_')[1] == 'vindr':\n",
        "                self.data['one_hot'] = list(np.hstack((np.zeros((y.shape[0], 1)), np.array(y))))\n",
        "\n",
        "            else:\n",
        "                self.data['one_hot'] = list(np.array(y))\n",
        "        else:\n",
        "            self.data['one_hot'] = list(np.array(y))\n",
        "        \n",
        "        self.image_root_dir = image_root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if t.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_path = os.path.join(self.image_root_dir,\n",
        "                                str(self.data.iloc[idx, 0]))\n",
        "\n",
        "        LCC =  io.read_image(img_path + '_LCC.png')\n",
        "        LMLO = io.read_image(img_path + '_LMLO.png')\n",
        "        RCC =  io.read_image(img_path + '_RCC.png')\n",
        "        RMLO = io.read_image(img_path + '_RMLO.png')\n",
        "\n",
        "        if LCC.shape[0] != 3:\n",
        "          LCC = t.cat([LCC, LCC, LCC], dim=0)\n",
        "          RCC = t.cat([RCC, RCC, RCC], dim=0)\n",
        "          LMLO = t.cat([LMLO, LMLO, LMLO], dim=0)\n",
        "          RMLO = t.cat([RMLO, RMLO, RMLO], dim=0)\n",
        "\n",
        "        if self.transform:\n",
        "            LCC = self.transform(LCC)\n",
        "            RCC = self.transform(RCC)\n",
        "            LMLO = self.transform(LMLO)\n",
        "            RMLO = self.transform(RMLO)\n",
        "\n",
        "        label = self.data.iloc[idx, 1]\n",
        "\n",
        "        sample = {'LCC': LCC, 'LMLO': LMLO, 'RCC': RCC, 'RMLO': RMLO,\n",
        "                  'label': label}\n",
        "                  \n",
        "        return sample\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, df, transform):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if t.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        image_path = self.df.iloc[idx, -1]\n",
        "        LCC =  io.read_image(image_path + '_LCC.png')\n",
        "        LMLO = io.read_image(image_path + '_LMLO.png')\n",
        "        RCC =  io.read_image(image_path + '_RCC.png')\n",
        "        RMLO = io.read_image(image_path + '_RMLO.png')\n",
        "\n",
        "        if LCC.shape[0] != 3:\n",
        "          LCC = t.cat([LCC, LCC, LCC], dim=0)\n",
        "          RCC = t.cat([RCC, RCC, RCC], dim=0)\n",
        "          LMLO = t.cat([LMLO, LMLO, LMLO], dim=0)\n",
        "          RMLO = t.cat([RMLO, RMLO, RMLO], dim=0)\n",
        "\n",
        "        label = self.df.iloc[idx, 2]\n",
        "\n",
        "        if self.transform:\n",
        "            LCC = self.transform(LCC)\n",
        "            RCC = self.transform(RCC)\n",
        "            LMLO = self.transform(LMLO)\n",
        "            RMLO = self.transform(RMLO)\n",
        "      \n",
        "\n",
        "        sample = {'LCC': LCC, 'LMLO': LMLO, 'RCC': RCC, 'RMLO': RMLO, 'label': label}\n",
        "        return sample"
      ],
      "metadata": {
        "id": "CcUP2v8Rn8p_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FocalLoss(nn.modules.loss._WeightedLoss):\n",
        "    def __init__(self, weight=None, gamma=2, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__(weight,reduction=reduction)\n",
        "        self.gamma = gamma\n",
        "        self.weight = weight #weight parameter will act as the alpha parameter to balance class weights\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        ce_loss = F.cross_entropy(input, target, reduction=self.reduction, weight=self.weight)\n",
        "        pt = t.exp(-ce_loss)\n",
        "        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
        "        return focal_loss"
      ],
      "metadata": {
        "id": "qVU_bnj1n-Ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class AttentionFuser(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AttentionFuser, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        # Define the linear layers for query, key, and value vectors\n",
        "        self.query_linear = nn.Linear(384, hidden_size)\n",
        "        self.key_linear = nn.Linear(384, hidden_size)\n",
        "        self.value_linear = nn.Linear(384, hidden_size)\n",
        "        \n",
        "    def forward(self, extractor_outputs):\n",
        "        # Compute the attention weights for each vector\n",
        "        attn_weights = []\n",
        "        for x in extractor_outputs:\n",
        "            # Apply the linear layers to reduce the dimensionality of the input vector\n",
        "            query = self.query_linear(x)\n",
        "            key = self.key_linear(x)\n",
        "            value = self.value_linear(x)\n",
        "\n",
        "            # Compute the attention scores\n",
        "            attn_scores = torch.bmm(query, key.transpose(1, 2))\n",
        "            attn_weights.append(F.softmax(attn_scores, dim=2))\n",
        "\n",
        "        # Combine the attention weights with the corresponding vectors\n",
        "        fused_output = torch.zeros((extractor_outputs[0].shape[0], 1, self.hidden_size), device=extractor_outputs[0].device)\n",
        "        for i, x in enumerate(extractor_outputs):\n",
        "            weighted_x = torch.bmm(attn_weights[i], value)\n",
        "            fused_output += weighted_x\n",
        "\n",
        "        return fused_output\n",
        "\n",
        "class GlobalTransformerEncoder(nn.Module):\n",
        "    def __init__(self, input_size, num_layers, num_heads, hidden_size, dropout_rate):\n",
        "        super(GlobalTransformerEncoder, self).__init__()\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(input_size, num_heads, hidden_size, dropout_rate),\n",
        "            num_layers\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.transformer_encoder(x)\n",
        "        return x\n",
        "\n",
        "class DortBasliMemeNet(nn.Module):\n",
        "    def __init__(self, f_extractor:str, fc:nn.Sequential, fuse):\n",
        "        super().__init__()\n",
        "\n",
        "        self.Extractor = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')\n",
        "        #self.MLO_Extractor = timm.create_model(f_extractor, pretrained=True, num_classes=0)\n",
        "        self.fuse = fuse\n",
        "        self.fc = fc\n",
        "\n",
        "    def forward(self, X):\n",
        "        LCC, LMLO, RCC, RMLO = X['LCC'].to(device), X['LMLO'].to(device), X['RCC'].to(device), X['RMLO'].to(device)\n",
        "        fLCC = self.Extractor(LCC)\n",
        "        fRCC = self.Extractor(RCC)\n",
        "        fLMLO = self.Extractor(LMLO)\n",
        "        fRMLO = self.Extractor(RMLO)\n",
        "      \n",
        "        # exct = torch.cat()\n",
        "\n",
        "        fused = t.cat((fLCC, fLMLO, fRCC, fRMLO), dim = -1)\n",
        "        #fused = [LCC.unsqueeze(1), LMLO.unsqueeze(1), RCC.unsqueeze(1), RMLO.unsqueeze(1)]\n",
        "        # print(fused.shape)\n",
        "        #fused = self.fuse(fused)\n",
        "        # print(fused.shape)\n",
        "        return fused\n",
        "\n",
        "transform_0 = T.Compose([\n",
        "    T.AutoAugment(T.AutoAugmentPolicy.IMAGENET),\n",
        "    T.ConvertImageDtype(t.float)\n",
        "])\n",
        "#  = T.Compose([\n",
        "#     T.GaussianBlur((3, 5)),\n",
        "#     T.RandomAffine((-30, 30), translate=(0.1, 0.3), scale=(0.5, 0.75)),\n",
        "#     T.RandomInvert(p = .2),\n",
        "#     T.ElasticTransform(),\n",
        "#     T.RandomErasing(p = .25),\n",
        "#     T.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "#     T.ConvertImageDtype(t.float),\n",
        "# ])\n",
        "\n",
        "transform = transform_0#T.Compose([\n",
        "#     T.RandomRotation((-30, 30)),\n",
        "#     T.GaussianBlur((5, 9)),\n",
        "#     T.ConvertImageDtype(t.float)\n",
        "# ])\n",
        "\n",
        "transform_test = T.Compose([\n",
        "    T.ConvertImageDtype(t.float)    \n",
        "])\n",
        "\n",
        "tasks = ('birads', 'kompozisyon')\n",
        "types = ('BIRADS KATEGORİSİ', 'MEME KOMPOZİSYONU')\n",
        "\n",
        "train_sets, val_sets = [], []\n",
        "\n",
        "for i in data_list:\n",
        "    dataset = MemeDataset(image_root_dir=f'processed_{i}', transform=transform, csv_file = f'{i}_{tasks[task]}_fixed.csv', model_type = types[task])\n",
        "\n",
        "    # convert data to DataFrame\n",
        "    df = dataset.data\n",
        "    df[\"image_path\"] = df[\"HASTANO\"].apply(lambda x: os.path.join(dataset.image_root_dir, str(x)))\n",
        "\n",
        "    # split data in a stratified way\n",
        "    if i == 'teknofest':\n",
        "        train_df, test_val_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df[types[task]])\n",
        "        val_df, test_df = train_test_split(test_val_df, test_size=0.5, random_state=42, stratify=test_val_df[types[task]])\n",
        "        test_set = CustomDataset(test_df, transform=transform_test)\n",
        "    \n",
        "    else:\n",
        "      if i == \"vindr\":\n",
        "          df = df[df[\"BIRADS KATEGORİSİ\"] != \"BI-RADS1-2\"]\n",
        "      train_df, val_df = train_test_split(df, test_size=0.15, random_state=42, stratify=df[types[task]])\n",
        "        \n",
        "    \n",
        "    train_sets.append(CustomDataset(train_df, transform=transform))\n",
        "    val_sets.append(CustomDataset(val_df, transform=transform_test))\n",
        "\n",
        "train_set = ConcatDataset(train_sets)\n",
        "val_set = ConcatDataset(val_sets)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size, True)\n",
        "val_loader = DataLoader(val_set, batch_size, False)\n",
        "test_loader = DataLoader(test_set, batch_size, False)\n",
        "\n",
        "num_class = train_sets[0].df['one_hot'].iloc[0].shape[0]\n",
        "counts = np.zeros(num_class)\n",
        "for x in train_sets:\n",
        "    bi_mat = x.df[\"one_hot\"].to_numpy()\n",
        "    for i in bi_mat:\n",
        "        ind = np.argmax(i)\n",
        "        counts[ind] += 1\n",
        "\n",
        "print(counts)\n",
        "\n",
        "a = np.array(counts)\n",
        "b = np.sum(a) / (num_class * a)\n",
        "class_weights = t.tensor(b, dtype=t.float)\n",
        "class_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02mIvuoLoPGM",
        "outputId": "83f1002d-6611-4b92-8b9f-5d122f7b6371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1527. 3101. 2886.]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.6403, 0.8077, 0.8679])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fuse = DepthwiseConv(6144, 512)\n",
        "# fuse = nn.Identity()\n",
        "fuse = nn.Identity()\n",
        "#fuse = AttentionFuser(224)\n",
        "\n",
        "fc_birads = fuse\n",
        "model = DortBasliMemeNet(model_name, fc_birads, fuse).to(device)\n",
        "\n",
        "tot_param = 0\n",
        "for params in model.parameters():\n",
        "  tot_param += params.numel()\n",
        "print(f\"The model has {tot_param:,} parameters.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9KQxeraqW-m",
        "outputId": "baa86cff-f766-4658-bb43-1cfd0103f90d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n",
            "WARNING:dinov2:xFormers not available\n",
            "WARNING:dinov2:xFormers not available\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dinov2_vitb14_pretrain.pth\n",
            "100%|██████████| 330M/330M [00:01<00:00, 208MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 86,580,480 parameters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total = len(train_loader)\n",
        "\n",
        "test_total = len(test_loader)\n",
        "# we will generate 10 times more training data than the original training data size using data augmentation\n",
        "train_features = []\n",
        "train_labels = []\n",
        "val_features = []\n",
        "val_labels = []\n",
        "test_features = []\n",
        "test_labels = []\n",
        "\n",
        "model.eval()\n",
        "index  = 0 \n",
        "for e in range(1):\n",
        "    for sample in tqdm(train_loader):\n",
        "        label = sample['label'].to(device)\n",
        "        out =  model(sample).detach().cpu().numpy()\n",
        "        # each generated train feature will be a 512 element vector\n",
        "        train_features.append(out)\n",
        "        # labels is in GPU, so transfer it to CPU first to assign to the numpy array\n",
        "        train_labels.append(label.cpu())\n",
        "        index += len(sample)\n",
        "\n",
        "index = 0\n",
        "for sample in tqdm(val_loader):\n",
        "    labels = sample['label'].to(device)\n",
        "    val_features.append(model(sample).detach().cpu().numpy())\n",
        "    val_labels.append(labels.cpu())\n",
        "    index += len(sample)\n",
        "\n",
        "index = 0\n",
        "for sample in tqdm(test_loader):\n",
        "    labels = sample['label'].to(device)\n",
        "    test_features.append(model(sample).detach().cpu().numpy())\n",
        "    test_labels.append(labels.cpu())\n",
        "    index += len(sample)\n",
        "\n",
        "train_features = np.concatenate(train_features)\n",
        "train_labels = np.concatenate(train_labels)\n",
        "val_features = np.concatenate(val_features)\n",
        "val_labels = np.concatenate(val_labels)\n",
        "test_features = np.concatenate(test_features)\n",
        "test_labels = np.concatenate(test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7Vx7d3DqFrj",
        "outputId": "594fc74f-3e2a-40cb-e574-c0fdc50f1c76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 470/470 [09:05<00:00,  1.16s/it]\n",
            "100%|██████████| 101/101 [01:43<00:00,  1.02s/it]\n",
            "100%|██████████| 101/101 [01:42<00:00,  1.02s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "num_feat = out.shape[-1]\n",
        "# Define the data and labels\n",
        "train_features_flatten = train_features.reshape(-1, num_feat)\n",
        "train_labels_flatten = np.argmax(train_labels, axis = 1)\n",
        "val_features_flatten = val_features.reshape(-1, num_feat)\n",
        "val_labels_flatten = np.argmax(val_labels, axis = 1)\n",
        "test_features_flatten = test_features.reshape(-1, num_feat)\n",
        "test_labels_flatten = np.argmax(test_labels, axis = 1)\n",
        "\n",
        "lgb_data_train = lgb.Dataset(train_features_flatten, label=train_labels_flatten)\n",
        "lgb_data_val = lgb.Dataset(val_features_flatten, label=val_labels_flatten)\n",
        "lgb_data_test = lgb.Dataset(test_features_flatten, label=test_labels_flatten)\n",
        "\n",
        "# Set up hyperparameters for the LightGBM model\n",
        "params = {\n",
        "    \"objective\": \"multiclass\",\n",
        "    \"num_class\": 3,\n",
        "    \"metric\": \"multi_logloss\",\n",
        "    \"boosting_type\": \"gbdt\",\n",
        "    \"learning_rate\": 0.01,\n",
        "    \"num_leaves\": 21,\n",
        "    \"max_depth\": -1,\n",
        "    \"min_child_samples\": 30,\n",
        "    \"subsample\": 1.0,\n",
        "    \"subsample_freq\": 0,\n",
        "    \"colsample_bytree\": 0.8, # 0-1 arası dişi 0.6 falan yap\n",
        "    \"reg_alpha\": 0.1,\n",
        "    \"reg_lambda\": 0.1,\n",
        "    \"n_jobs\": -1,\n",
        "    \"verbose\": 1\n",
        "}\n",
        "\n",
        "# Train the LightGBM model\n",
        "num_rounds = 1000\n",
        "model = lgb.train(params, lgb_data_train, num_rounds, valid_sets = [lgb_data_val], early_stopping_rounds=10, verbose_eval=True)\n",
        "\n",
        "# Evaluate the model on the training data\n",
        "predictions = model.predict(train_features_flatten)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "accuracy = np.sum(predicted_labels == train_labels_flatten) / len(train_labels_flatten)\n",
        "print(f\"Train Accuracy: {accuracy}\")\n",
        "\n",
        "predictions = model.predict(val_features_flatten)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "accuracy = np.sum(predicted_labels == val_labels_flatten) / len(val_labels_flatten)\n",
        "print(f\"Validation Accuracy: {accuracy}\")\n",
        "\n",
        "predictions = model.predict(test_features_flatten)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "accuracy = np.sum(predicted_labels == test_labels_flatten) / len(test_labels_flatten)\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dx_G4YhQsFcJ",
        "outputId": "691a911e-b5bd-4d4f-ecdb-0c8f52aff4c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.812351 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 783360\n",
            "[LightGBM] [Info] Number of data points in the train set: 7514, number of used features: 3072\n",
            "[LightGBM] [Info] Start training from score -1.593463\n",
            "[LightGBM] [Info] Start training from score -0.885043\n",
            "[LightGBM] [Info] Start training from score -0.956896\n",
            "[1]\tvalid_0's multi_logloss: 1.0556\n",
            "Training until validation scores don't improve for 10 rounds\n",
            "[2]\tvalid_0's multi_logloss: 1.05468\n",
            "[3]\tvalid_0's multi_logloss: 1.05384\n",
            "[4]\tvalid_0's multi_logloss: 1.05295\n",
            "[5]\tvalid_0's multi_logloss: 1.05205\n",
            "[6]\tvalid_0's multi_logloss: 1.05118\n",
            "[7]\tvalid_0's multi_logloss: 1.05041\n",
            "[8]\tvalid_0's multi_logloss: 1.04952\n",
            "[9]\tvalid_0's multi_logloss: 1.04866\n",
            "[10]\tvalid_0's multi_logloss: 1.04779\n",
            "[11]\tvalid_0's multi_logloss: 1.04703\n",
            "[12]\tvalid_0's multi_logloss: 1.04623\n",
            "[13]\tvalid_0's multi_logloss: 1.04522\n",
            "[14]\tvalid_0's multi_logloss: 1.04459\n",
            "[15]\tvalid_0's multi_logloss: 1.04384\n",
            "[16]\tvalid_0's multi_logloss: 1.04319\n",
            "[17]\tvalid_0's multi_logloss: 1.0425\n",
            "[18]\tvalid_0's multi_logloss: 1.04182\n",
            "[19]\tvalid_0's multi_logloss: 1.04104\n",
            "[20]\tvalid_0's multi_logloss: 1.04039\n",
            "[21]\tvalid_0's multi_logloss: 1.03966\n",
            "[22]\tvalid_0's multi_logloss: 1.03889\n",
            "[23]\tvalid_0's multi_logloss: 1.03826\n",
            "[24]\tvalid_0's multi_logloss: 1.03767\n",
            "[25]\tvalid_0's multi_logloss: 1.03696\n",
            "[26]\tvalid_0's multi_logloss: 1.03618\n",
            "[27]\tvalid_0's multi_logloss: 1.03551\n",
            "[28]\tvalid_0's multi_logloss: 1.03489\n",
            "[29]\tvalid_0's multi_logloss: 1.03423\n",
            "[30]\tvalid_0's multi_logloss: 1.03359\n",
            "[31]\tvalid_0's multi_logloss: 1.03293\n",
            "[32]\tvalid_0's multi_logloss: 1.03216\n",
            "[33]\tvalid_0's multi_logloss: 1.03146\n",
            "[34]\tvalid_0's multi_logloss: 1.03094\n",
            "[35]\tvalid_0's multi_logloss: 1.03029\n",
            "[36]\tvalid_0's multi_logloss: 1.02967\n",
            "[37]\tvalid_0's multi_logloss: 1.02904\n",
            "[38]\tvalid_0's multi_logloss: 1.02835\n",
            "[39]\tvalid_0's multi_logloss: 1.02777\n",
            "[40]\tvalid_0's multi_logloss: 1.02712\n",
            "[41]\tvalid_0's multi_logloss: 1.0265\n",
            "[42]\tvalid_0's multi_logloss: 1.02597\n",
            "[43]\tvalid_0's multi_logloss: 1.02552\n",
            "[44]\tvalid_0's multi_logloss: 1.02486\n",
            "[45]\tvalid_0's multi_logloss: 1.02432\n",
            "[46]\tvalid_0's multi_logloss: 1.02374\n",
            "[47]\tvalid_0's multi_logloss: 1.02325\n",
            "[48]\tvalid_0's multi_logloss: 1.02265\n",
            "[49]\tvalid_0's multi_logloss: 1.02206\n",
            "[50]\tvalid_0's multi_logloss: 1.02164\n",
            "[51]\tvalid_0's multi_logloss: 1.02114\n",
            "[52]\tvalid_0's multi_logloss: 1.02052\n",
            "[53]\tvalid_0's multi_logloss: 1.0199\n",
            "[54]\tvalid_0's multi_logloss: 1.01935\n",
            "[55]\tvalid_0's multi_logloss: 1.01892\n",
            "[56]\tvalid_0's multi_logloss: 1.01844\n",
            "[57]\tvalid_0's multi_logloss: 1.01792\n",
            "[58]\tvalid_0's multi_logloss: 1.01731\n",
            "[59]\tvalid_0's multi_logloss: 1.01678\n",
            "[60]\tvalid_0's multi_logloss: 1.01625\n",
            "[61]\tvalid_0's multi_logloss: 1.01581\n",
            "[62]\tvalid_0's multi_logloss: 1.0153\n",
            "[63]\tvalid_0's multi_logloss: 1.01481\n",
            "[64]\tvalid_0's multi_logloss: 1.01432\n",
            "[65]\tvalid_0's multi_logloss: 1.0138\n",
            "[66]\tvalid_0's multi_logloss: 1.01339\n",
            "[67]\tvalid_0's multi_logloss: 1.0129\n",
            "[68]\tvalid_0's multi_logloss: 1.01238\n",
            "[69]\tvalid_0's multi_logloss: 1.01198\n",
            "[70]\tvalid_0's multi_logloss: 1.01153\n",
            "[71]\tvalid_0's multi_logloss: 1.01108\n",
            "[72]\tvalid_0's multi_logloss: 1.01063\n",
            "[73]\tvalid_0's multi_logloss: 1.01015\n",
            "[74]\tvalid_0's multi_logloss: 1.00976\n",
            "[75]\tvalid_0's multi_logloss: 1.00931\n",
            "[76]\tvalid_0's multi_logloss: 1.00883\n",
            "[77]\tvalid_0's multi_logloss: 1.00834\n",
            "[78]\tvalid_0's multi_logloss: 1.00805\n",
            "[79]\tvalid_0's multi_logloss: 1.00759\n",
            "[80]\tvalid_0's multi_logloss: 1.00725\n",
            "[81]\tvalid_0's multi_logloss: 1.00688\n",
            "[82]\tvalid_0's multi_logloss: 1.0065\n",
            "[83]\tvalid_0's multi_logloss: 1.00608\n",
            "[84]\tvalid_0's multi_logloss: 1.00572\n",
            "[85]\tvalid_0's multi_logloss: 1.00535\n",
            "[86]\tvalid_0's multi_logloss: 1.00496\n",
            "[87]\tvalid_0's multi_logloss: 1.00466\n",
            "[88]\tvalid_0's multi_logloss: 1.00427\n",
            "[89]\tvalid_0's multi_logloss: 1.00385\n",
            "[90]\tvalid_0's multi_logloss: 1.00353\n",
            "[91]\tvalid_0's multi_logloss: 1.00312\n",
            "[92]\tvalid_0's multi_logloss: 1.00276\n",
            "[93]\tvalid_0's multi_logloss: 1.00241\n",
            "[94]\tvalid_0's multi_logloss: 1.00208\n",
            "[95]\tvalid_0's multi_logloss: 1.00174\n",
            "[96]\tvalid_0's multi_logloss: 1.0013\n",
            "[97]\tvalid_0's multi_logloss: 1.0009\n",
            "[98]\tvalid_0's multi_logloss: 1.00049\n",
            "[99]\tvalid_0's multi_logloss: 1.00015\n",
            "[100]\tvalid_0's multi_logloss: 0.999868\n",
            "[101]\tvalid_0's multi_logloss: 0.999612\n",
            "[102]\tvalid_0's multi_logloss: 0.999323\n",
            "[103]\tvalid_0's multi_logloss: 0.99897\n",
            "[104]\tvalid_0's multi_logloss: 0.998566\n",
            "[105]\tvalid_0's multi_logloss: 0.998267\n",
            "[106]\tvalid_0's multi_logloss: 0.997948\n",
            "[107]\tvalid_0's multi_logloss: 0.997592\n",
            "[108]\tvalid_0's multi_logloss: 0.997203\n",
            "[109]\tvalid_0's multi_logloss: 0.996908\n",
            "[110]\tvalid_0's multi_logloss: 0.996574\n",
            "[111]\tvalid_0's multi_logloss: 0.996198\n",
            "[112]\tvalid_0's multi_logloss: 0.995799\n",
            "[113]\tvalid_0's multi_logloss: 0.995482\n",
            "[114]\tvalid_0's multi_logloss: 0.995154\n",
            "[115]\tvalid_0's multi_logloss: 0.994872\n",
            "[116]\tvalid_0's multi_logloss: 0.994468\n",
            "[117]\tvalid_0's multi_logloss: 0.994143\n",
            "[118]\tvalid_0's multi_logloss: 0.993881\n",
            "[119]\tvalid_0's multi_logloss: 0.993552\n",
            "[120]\tvalid_0's multi_logloss: 0.993181\n",
            "[121]\tvalid_0's multi_logloss: 0.992799\n",
            "[122]\tvalid_0's multi_logloss: 0.992485\n",
            "[123]\tvalid_0's multi_logloss: 0.992251\n",
            "[124]\tvalid_0's multi_logloss: 0.991991\n",
            "[125]\tvalid_0's multi_logloss: 0.991633\n",
            "[126]\tvalid_0's multi_logloss: 0.991388\n",
            "[127]\tvalid_0's multi_logloss: 0.99105\n",
            "[128]\tvalid_0's multi_logloss: 0.990825\n",
            "[129]\tvalid_0's multi_logloss: 0.990613\n",
            "[130]\tvalid_0's multi_logloss: 0.990294\n",
            "[131]\tvalid_0's multi_logloss: 0.990003\n",
            "[132]\tvalid_0's multi_logloss: 0.989755\n",
            "[133]\tvalid_0's multi_logloss: 0.989434\n",
            "[134]\tvalid_0's multi_logloss: 0.989178\n",
            "[135]\tvalid_0's multi_logloss: 0.988901\n",
            "[136]\tvalid_0's multi_logloss: 0.988552\n",
            "[137]\tvalid_0's multi_logloss: 0.988268\n",
            "[138]\tvalid_0's multi_logloss: 0.988072\n",
            "[139]\tvalid_0's multi_logloss: 0.987806\n",
            "[140]\tvalid_0's multi_logloss: 0.987572\n",
            "[141]\tvalid_0's multi_logloss: 0.987337\n",
            "[142]\tvalid_0's multi_logloss: 0.986973\n",
            "[143]\tvalid_0's multi_logloss: 0.986791\n",
            "[144]\tvalid_0's multi_logloss: 0.986543\n",
            "[145]\tvalid_0's multi_logloss: 0.986197\n",
            "[146]\tvalid_0's multi_logloss: 0.985996\n",
            "[147]\tvalid_0's multi_logloss: 0.985852\n",
            "[148]\tvalid_0's multi_logloss: 0.985659\n",
            "[149]\tvalid_0's multi_logloss: 0.985382\n",
            "[150]\tvalid_0's multi_logloss: 0.985215\n",
            "[151]\tvalid_0's multi_logloss: 0.984848\n",
            "[152]\tvalid_0's multi_logloss: 0.984671\n",
            "[153]\tvalid_0's multi_logloss: 0.98432\n",
            "[154]\tvalid_0's multi_logloss: 0.984007\n",
            "[155]\tvalid_0's multi_logloss: 0.983739\n",
            "[156]\tvalid_0's multi_logloss: 0.983524\n",
            "[157]\tvalid_0's multi_logloss: 0.983323\n",
            "[158]\tvalid_0's multi_logloss: 0.983047\n",
            "[159]\tvalid_0's multi_logloss: 0.982795\n",
            "[160]\tvalid_0's multi_logloss: 0.982594\n",
            "[161]\tvalid_0's multi_logloss: 0.982371\n",
            "[162]\tvalid_0's multi_logloss: 0.982145\n",
            "[163]\tvalid_0's multi_logloss: 0.981979\n",
            "[164]\tvalid_0's multi_logloss: 0.981689\n",
            "[165]\tvalid_0's multi_logloss: 0.981487\n",
            "[166]\tvalid_0's multi_logloss: 0.981143\n",
            "[167]\tvalid_0's multi_logloss: 0.980976\n",
            "[168]\tvalid_0's multi_logloss: 0.980702\n",
            "[169]\tvalid_0's multi_logloss: 0.980551\n",
            "[170]\tvalid_0's multi_logloss: 0.980337\n",
            "[171]\tvalid_0's multi_logloss: 0.980215\n",
            "[172]\tvalid_0's multi_logloss: 0.980026\n",
            "[173]\tvalid_0's multi_logloss: 0.979765\n",
            "[174]\tvalid_0's multi_logloss: 0.979491\n",
            "[175]\tvalid_0's multi_logloss: 0.979195\n",
            "[176]\tvalid_0's multi_logloss: 0.979039\n",
            "[177]\tvalid_0's multi_logloss: 0.978745\n",
            "[178]\tvalid_0's multi_logloss: 0.978389\n",
            "[179]\tvalid_0's multi_logloss: 0.978197\n",
            "[180]\tvalid_0's multi_logloss: 0.977974\n",
            "[181]\tvalid_0's multi_logloss: 0.977698\n",
            "[182]\tvalid_0's multi_logloss: 0.977446\n",
            "[183]\tvalid_0's multi_logloss: 0.977191\n",
            "[184]\tvalid_0's multi_logloss: 0.976884\n",
            "[185]\tvalid_0's multi_logloss: 0.976664\n",
            "[186]\tvalid_0's multi_logloss: 0.976317\n",
            "[187]\tvalid_0's multi_logloss: 0.976153\n",
            "[188]\tvalid_0's multi_logloss: 0.975889\n",
            "[189]\tvalid_0's multi_logloss: 0.975753\n",
            "[190]\tvalid_0's multi_logloss: 0.975593\n",
            "[191]\tvalid_0's multi_logloss: 0.975312\n",
            "[192]\tvalid_0's multi_logloss: 0.975128\n",
            "[193]\tvalid_0's multi_logloss: 0.974922\n",
            "[194]\tvalid_0's multi_logloss: 0.974657\n",
            "[195]\tvalid_0's multi_logloss: 0.974364\n",
            "[196]\tvalid_0's multi_logloss: 0.974053\n",
            "[197]\tvalid_0's multi_logloss: 0.973727\n",
            "[198]\tvalid_0's multi_logloss: 0.973595\n",
            "[199]\tvalid_0's multi_logloss: 0.973309\n",
            "[200]\tvalid_0's multi_logloss: 0.973125\n",
            "[201]\tvalid_0's multi_logloss: 0.972977\n",
            "[202]\tvalid_0's multi_logloss: 0.972864\n",
            "[203]\tvalid_0's multi_logloss: 0.97266\n",
            "[204]\tvalid_0's multi_logloss: 0.972433\n",
            "[205]\tvalid_0's multi_logloss: 0.972346\n",
            "[206]\tvalid_0's multi_logloss: 0.972089\n",
            "[207]\tvalid_0's multi_logloss: 0.971865\n",
            "[208]\tvalid_0's multi_logloss: 0.97174\n",
            "[209]\tvalid_0's multi_logloss: 0.971707\n",
            "[210]\tvalid_0's multi_logloss: 0.971529\n",
            "[211]\tvalid_0's multi_logloss: 0.97135\n",
            "[212]\tvalid_0's multi_logloss: 0.971118\n",
            "[213]\tvalid_0's multi_logloss: 0.970958\n",
            "[214]\tvalid_0's multi_logloss: 0.970751\n",
            "[215]\tvalid_0's multi_logloss: 0.970533\n",
            "[216]\tvalid_0's multi_logloss: 0.970301\n",
            "[217]\tvalid_0's multi_logloss: 0.970008\n",
            "[218]\tvalid_0's multi_logloss: 0.9699\n",
            "[219]\tvalid_0's multi_logloss: 0.969766\n",
            "[220]\tvalid_0's multi_logloss: 0.969645\n",
            "[221]\tvalid_0's multi_logloss: 0.969382\n",
            "[222]\tvalid_0's multi_logloss: 0.96912\n",
            "[223]\tvalid_0's multi_logloss: 0.96904\n",
            "[224]\tvalid_0's multi_logloss: 0.968933\n",
            "[225]\tvalid_0's multi_logloss: 0.968649\n",
            "[226]\tvalid_0's multi_logloss: 0.968452\n",
            "[227]\tvalid_0's multi_logloss: 0.968314\n",
            "[228]\tvalid_0's multi_logloss: 0.96802\n",
            "[229]\tvalid_0's multi_logloss: 0.967931\n",
            "[230]\tvalid_0's multi_logloss: 0.967762\n",
            "[231]\tvalid_0's multi_logloss: 0.967575\n",
            "[232]\tvalid_0's multi_logloss: 0.967389\n",
            "[233]\tvalid_0's multi_logloss: 0.967256\n",
            "[234]\tvalid_0's multi_logloss: 0.967142\n",
            "[235]\tvalid_0's multi_logloss: 0.966854\n",
            "[236]\tvalid_0's multi_logloss: 0.966752\n",
            "[237]\tvalid_0's multi_logloss: 0.966678\n",
            "[238]\tvalid_0's multi_logloss: 0.966546\n",
            "[239]\tvalid_0's multi_logloss: 0.966419\n",
            "[240]\tvalid_0's multi_logloss: 0.966392\n",
            "[241]\tvalid_0's multi_logloss: 0.966275\n",
            "[242]\tvalid_0's multi_logloss: 0.966198\n",
            "[243]\tvalid_0's multi_logloss: 0.965981\n",
            "[244]\tvalid_0's multi_logloss: 0.965853\n",
            "[245]\tvalid_0's multi_logloss: 0.965659\n",
            "[246]\tvalid_0's multi_logloss: 0.965532\n",
            "[247]\tvalid_0's multi_logloss: 0.965221\n",
            "[248]\tvalid_0's multi_logloss: 0.965072\n",
            "[249]\tvalid_0's multi_logloss: 0.964946\n",
            "[250]\tvalid_0's multi_logloss: 0.964725\n",
            "[251]\tvalid_0's multi_logloss: 0.964565\n",
            "[252]\tvalid_0's multi_logloss: 0.964445\n",
            "[253]\tvalid_0's multi_logloss: 0.964236\n",
            "[254]\tvalid_0's multi_logloss: 0.964096\n",
            "[255]\tvalid_0's multi_logloss: 0.964027\n",
            "[256]\tvalid_0's multi_logloss: 0.963856\n",
            "[257]\tvalid_0's multi_logloss: 0.963655\n",
            "[258]\tvalid_0's multi_logloss: 0.963576\n",
            "[259]\tvalid_0's multi_logloss: 0.963437\n",
            "[260]\tvalid_0's multi_logloss: 0.963311\n",
            "[261]\tvalid_0's multi_logloss: 0.963225\n",
            "[262]\tvalid_0's multi_logloss: 0.963098\n",
            "[263]\tvalid_0's multi_logloss: 0.962969\n",
            "[264]\tvalid_0's multi_logloss: 0.962831\n",
            "[265]\tvalid_0's multi_logloss: 0.962867\n",
            "[266]\tvalid_0's multi_logloss: 0.962686\n",
            "[267]\tvalid_0's multi_logloss: 0.962496\n",
            "[268]\tvalid_0's multi_logloss: 0.962457\n",
            "[269]\tvalid_0's multi_logloss: 0.962388\n",
            "[270]\tvalid_0's multi_logloss: 0.962303\n",
            "[271]\tvalid_0's multi_logloss: 0.962228\n",
            "[272]\tvalid_0's multi_logloss: 0.962114\n",
            "[273]\tvalid_0's multi_logloss: 0.961952\n",
            "[274]\tvalid_0's multi_logloss: 0.961877\n",
            "[275]\tvalid_0's multi_logloss: 0.961707\n",
            "[276]\tvalid_0's multi_logloss: 0.961625\n",
            "[277]\tvalid_0's multi_logloss: 0.961414\n",
            "[278]\tvalid_0's multi_logloss: 0.961328\n",
            "[279]\tvalid_0's multi_logloss: 0.961144\n",
            "[280]\tvalid_0's multi_logloss: 0.960969\n",
            "[281]\tvalid_0's multi_logloss: 0.960821\n",
            "[282]\tvalid_0's multi_logloss: 0.960763\n",
            "[283]\tvalid_0's multi_logloss: 0.960698\n",
            "[284]\tvalid_0's multi_logloss: 0.960571\n",
            "[285]\tvalid_0's multi_logloss: 0.960499\n",
            "[286]\tvalid_0's multi_logloss: 0.960365\n",
            "[287]\tvalid_0's multi_logloss: 0.960222\n",
            "[288]\tvalid_0's multi_logloss: 0.960084\n",
            "[289]\tvalid_0's multi_logloss: 0.96007\n",
            "[290]\tvalid_0's multi_logloss: 0.959954\n",
            "[291]\tvalid_0's multi_logloss: 0.95986\n",
            "[292]\tvalid_0's multi_logloss: 0.95981\n",
            "[293]\tvalid_0's multi_logloss: 0.959691\n",
            "[294]\tvalid_0's multi_logloss: 0.959453\n",
            "[295]\tvalid_0's multi_logloss: 0.95936\n",
            "[296]\tvalid_0's multi_logloss: 0.95919\n",
            "[297]\tvalid_0's multi_logloss: 0.959103\n",
            "[298]\tvalid_0's multi_logloss: 0.958925\n",
            "[299]\tvalid_0's multi_logloss: 0.958844\n",
            "[300]\tvalid_0's multi_logloss: 0.958737\n",
            "[301]\tvalid_0's multi_logloss: 0.958552\n",
            "[302]\tvalid_0's multi_logloss: 0.958428\n",
            "[303]\tvalid_0's multi_logloss: 0.958386\n",
            "[304]\tvalid_0's multi_logloss: 0.958224\n",
            "[305]\tvalid_0's multi_logloss: 0.958201\n",
            "[306]\tvalid_0's multi_logloss: 0.95814\n",
            "[307]\tvalid_0's multi_logloss: 0.958138\n",
            "[308]\tvalid_0's multi_logloss: 0.958087\n",
            "[309]\tvalid_0's multi_logloss: 0.958028\n",
            "[310]\tvalid_0's multi_logloss: 0.957917\n",
            "[311]\tvalid_0's multi_logloss: 0.957801\n",
            "[312]\tvalid_0's multi_logloss: 0.957676\n",
            "[313]\tvalid_0's multi_logloss: 0.957558\n",
            "[314]\tvalid_0's multi_logloss: 0.957493\n",
            "[315]\tvalid_0's multi_logloss: 0.957339\n",
            "[316]\tvalid_0's multi_logloss: 0.957272\n",
            "[317]\tvalid_0's multi_logloss: 0.957339\n",
            "[318]\tvalid_0's multi_logloss: 0.957246\n",
            "[319]\tvalid_0's multi_logloss: 0.957014\n",
            "[320]\tvalid_0's multi_logloss: 0.956869\n",
            "[321]\tvalid_0's multi_logloss: 0.956722\n",
            "[322]\tvalid_0's multi_logloss: 0.95659\n",
            "[323]\tvalid_0's multi_logloss: 0.95651\n",
            "[324]\tvalid_0's multi_logloss: 0.956415\n",
            "[325]\tvalid_0's multi_logloss: 0.956418\n",
            "[326]\tvalid_0's multi_logloss: 0.956332\n",
            "[327]\tvalid_0's multi_logloss: 0.956277\n",
            "[328]\tvalid_0's multi_logloss: 0.956182\n",
            "[329]\tvalid_0's multi_logloss: 0.95609\n",
            "[330]\tvalid_0's multi_logloss: 0.956011\n",
            "[331]\tvalid_0's multi_logloss: 0.955986\n",
            "[332]\tvalid_0's multi_logloss: 0.955827\n",
            "[333]\tvalid_0's multi_logloss: 0.955749\n",
            "[334]\tvalid_0's multi_logloss: 0.955659\n",
            "[335]\tvalid_0's multi_logloss: 0.955539\n",
            "[336]\tvalid_0's multi_logloss: 0.955465\n",
            "[337]\tvalid_0's multi_logloss: 0.955391\n",
            "[338]\tvalid_0's multi_logloss: 0.955293\n",
            "[339]\tvalid_0's multi_logloss: 0.955209\n",
            "[340]\tvalid_0's multi_logloss: 0.95513\n",
            "[341]\tvalid_0's multi_logloss: 0.95506\n",
            "[342]\tvalid_0's multi_logloss: 0.954979\n",
            "[343]\tvalid_0's multi_logloss: 0.955013\n",
            "[344]\tvalid_0's multi_logloss: 0.954959\n",
            "[345]\tvalid_0's multi_logloss: 0.954934\n",
            "[346]\tvalid_0's multi_logloss: 0.954858\n",
            "[347]\tvalid_0's multi_logloss: 0.954722\n",
            "[348]\tvalid_0's multi_logloss: 0.954644\n",
            "[349]\tvalid_0's multi_logloss: 0.954548\n",
            "[350]\tvalid_0's multi_logloss: 0.954429\n",
            "[351]\tvalid_0's multi_logloss: 0.954368\n",
            "[352]\tvalid_0's multi_logloss: 0.954251\n",
            "[353]\tvalid_0's multi_logloss: 0.954097\n",
            "[354]\tvalid_0's multi_logloss: 0.954074\n",
            "[355]\tvalid_0's multi_logloss: 0.953966\n",
            "[356]\tvalid_0's multi_logloss: 0.953908\n",
            "[357]\tvalid_0's multi_logloss: 0.953858\n",
            "[358]\tvalid_0's multi_logloss: 0.953729\n",
            "[359]\tvalid_0's multi_logloss: 0.953641\n",
            "[360]\tvalid_0's multi_logloss: 0.953559\n",
            "[361]\tvalid_0's multi_logloss: 0.953409\n",
            "[362]\tvalid_0's multi_logloss: 0.953272\n",
            "[363]\tvalid_0's multi_logloss: 0.953139\n",
            "[364]\tvalid_0's multi_logloss: 0.95295\n",
            "[365]\tvalid_0's multi_logloss: 0.952914\n",
            "[366]\tvalid_0's multi_logloss: 0.952844\n",
            "[367]\tvalid_0's multi_logloss: 0.952661\n",
            "[368]\tvalid_0's multi_logloss: 0.952591\n",
            "[369]\tvalid_0's multi_logloss: 0.952522\n",
            "[370]\tvalid_0's multi_logloss: 0.952517\n",
            "[371]\tvalid_0's multi_logloss: 0.952433\n",
            "[372]\tvalid_0's multi_logloss: 0.952307\n",
            "[373]\tvalid_0's multi_logloss: 0.952338\n",
            "[374]\tvalid_0's multi_logloss: 0.952115\n",
            "[375]\tvalid_0's multi_logloss: 0.951979\n",
            "[376]\tvalid_0's multi_logloss: 0.951852\n",
            "[377]\tvalid_0's multi_logloss: 0.951743\n",
            "[378]\tvalid_0's multi_logloss: 0.951638\n",
            "[379]\tvalid_0's multi_logloss: 0.95143\n",
            "[380]\tvalid_0's multi_logloss: 0.951398\n",
            "[381]\tvalid_0's multi_logloss: 0.951337\n",
            "[382]\tvalid_0's multi_logloss: 0.951256\n",
            "[383]\tvalid_0's multi_logloss: 0.951217\n",
            "[384]\tvalid_0's multi_logloss: 0.951082\n",
            "[385]\tvalid_0's multi_logloss: 0.950877\n",
            "[386]\tvalid_0's multi_logloss: 0.950686\n",
            "[387]\tvalid_0's multi_logloss: 0.950577\n",
            "[388]\tvalid_0's multi_logloss: 0.950481\n",
            "[389]\tvalid_0's multi_logloss: 0.950418\n",
            "[390]\tvalid_0's multi_logloss: 0.950395\n",
            "[391]\tvalid_0's multi_logloss: 0.950371\n",
            "[392]\tvalid_0's multi_logloss: 0.950302\n",
            "[393]\tvalid_0's multi_logloss: 0.950179\n",
            "[394]\tvalid_0's multi_logloss: 0.950108\n",
            "[395]\tvalid_0's multi_logloss: 0.95003\n",
            "[396]\tvalid_0's multi_logloss: 0.949895\n",
            "[397]\tvalid_0's multi_logloss: 0.949886\n",
            "[398]\tvalid_0's multi_logloss: 0.949776\n",
            "[399]\tvalid_0's multi_logloss: 0.94969\n",
            "[400]\tvalid_0's multi_logloss: 0.949666\n",
            "[401]\tvalid_0's multi_logloss: 0.949577\n",
            "[402]\tvalid_0's multi_logloss: 0.949521\n",
            "[403]\tvalid_0's multi_logloss: 0.949443\n",
            "[404]\tvalid_0's multi_logloss: 0.949374\n",
            "[405]\tvalid_0's multi_logloss: 0.9493\n",
            "[406]\tvalid_0's multi_logloss: 0.949173\n",
            "[407]\tvalid_0's multi_logloss: 0.949105\n",
            "[408]\tvalid_0's multi_logloss: 0.94895\n",
            "[409]\tvalid_0's multi_logloss: 0.948824\n",
            "[410]\tvalid_0's multi_logloss: 0.948779\n",
            "[411]\tvalid_0's multi_logloss: 0.948725\n",
            "[412]\tvalid_0's multi_logloss: 0.948802\n",
            "[413]\tvalid_0's multi_logloss: 0.948764\n",
            "[414]\tvalid_0's multi_logloss: 0.948766\n",
            "[415]\tvalid_0's multi_logloss: 0.948774\n",
            "[416]\tvalid_0's multi_logloss: 0.948689\n",
            "[417]\tvalid_0's multi_logloss: 0.948652\n",
            "[418]\tvalid_0's multi_logloss: 0.948549\n",
            "[419]\tvalid_0's multi_logloss: 0.948472\n",
            "[420]\tvalid_0's multi_logloss: 0.948356\n",
            "[421]\tvalid_0's multi_logloss: 0.948293\n",
            "[422]\tvalid_0's multi_logloss: 0.948275\n",
            "[423]\tvalid_0's multi_logloss: 0.94822\n",
            "[424]\tvalid_0's multi_logloss: 0.948211\n",
            "[425]\tvalid_0's multi_logloss: 0.948221\n",
            "[426]\tvalid_0's multi_logloss: 0.948269\n",
            "[427]\tvalid_0's multi_logloss: 0.948204\n",
            "[428]\tvalid_0's multi_logloss: 0.948123\n",
            "[429]\tvalid_0's multi_logloss: 0.948117\n",
            "[430]\tvalid_0's multi_logloss: 0.948046\n",
            "[431]\tvalid_0's multi_logloss: 0.94792\n",
            "[432]\tvalid_0's multi_logloss: 0.947833\n",
            "[433]\tvalid_0's multi_logloss: 0.947817\n",
            "[434]\tvalid_0's multi_logloss: 0.947735\n",
            "[435]\tvalid_0's multi_logloss: 0.947699\n",
            "[436]\tvalid_0's multi_logloss: 0.947661\n",
            "[437]\tvalid_0's multi_logloss: 0.947633\n",
            "[438]\tvalid_0's multi_logloss: 0.947635\n",
            "[439]\tvalid_0's multi_logloss: 0.947518\n",
            "[440]\tvalid_0's multi_logloss: 0.947399\n",
            "[441]\tvalid_0's multi_logloss: 0.947312\n",
            "[442]\tvalid_0's multi_logloss: 0.947228\n",
            "[443]\tvalid_0's multi_logloss: 0.947212\n",
            "[444]\tvalid_0's multi_logloss: 0.947122\n",
            "[445]\tvalid_0's multi_logloss: 0.947013\n",
            "[446]\tvalid_0's multi_logloss: 0.947048\n",
            "[447]\tvalid_0's multi_logloss: 0.946934\n",
            "[448]\tvalid_0's multi_logloss: 0.946818\n",
            "[449]\tvalid_0's multi_logloss: 0.946734\n",
            "[450]\tvalid_0's multi_logloss: 0.94679\n",
            "[451]\tvalid_0's multi_logloss: 0.946852\n",
            "[452]\tvalid_0's multi_logloss: 0.946767\n",
            "[453]\tvalid_0's multi_logloss: 0.946705\n",
            "[454]\tvalid_0's multi_logloss: 0.946647\n",
            "[455]\tvalid_0's multi_logloss: 0.94654\n",
            "[456]\tvalid_0's multi_logloss: 0.946502\n",
            "[457]\tvalid_0's multi_logloss: 0.94641\n",
            "[458]\tvalid_0's multi_logloss: 0.946359\n",
            "[459]\tvalid_0's multi_logloss: 0.946318\n",
            "[460]\tvalid_0's multi_logloss: 0.946266\n",
            "[461]\tvalid_0's multi_logloss: 0.946173\n",
            "[462]\tvalid_0's multi_logloss: 0.946139\n",
            "[463]\tvalid_0's multi_logloss: 0.946056\n",
            "[464]\tvalid_0's multi_logloss: 0.945964\n",
            "[465]\tvalid_0's multi_logloss: 0.945905\n",
            "[466]\tvalid_0's multi_logloss: 0.945813\n",
            "[467]\tvalid_0's multi_logloss: 0.945823\n",
            "[468]\tvalid_0's multi_logloss: 0.945773\n",
            "[469]\tvalid_0's multi_logloss: 0.945817\n",
            "[470]\tvalid_0's multi_logloss: 0.945851\n",
            "[471]\tvalid_0's multi_logloss: 0.945783\n",
            "[472]\tvalid_0's multi_logloss: 0.945673\n",
            "[473]\tvalid_0's multi_logloss: 0.945675\n",
            "[474]\tvalid_0's multi_logloss: 0.945571\n",
            "[475]\tvalid_0's multi_logloss: 0.9455\n",
            "[476]\tvalid_0's multi_logloss: 0.945398\n",
            "[477]\tvalid_0's multi_logloss: 0.945379\n",
            "[478]\tvalid_0's multi_logloss: 0.945306\n",
            "[479]\tvalid_0's multi_logloss: 0.945298\n",
            "[480]\tvalid_0's multi_logloss: 0.945344\n",
            "[481]\tvalid_0's multi_logloss: 0.945242\n",
            "[482]\tvalid_0's multi_logloss: 0.945194\n",
            "[483]\tvalid_0's multi_logloss: 0.945099\n",
            "[484]\tvalid_0's multi_logloss: 0.945118\n",
            "[485]\tvalid_0's multi_logloss: 0.945068\n",
            "[486]\tvalid_0's multi_logloss: 0.944945\n",
            "[487]\tvalid_0's multi_logloss: 0.944879\n",
            "[488]\tvalid_0's multi_logloss: 0.944812\n",
            "[489]\tvalid_0's multi_logloss: 0.944767\n",
            "[490]\tvalid_0's multi_logloss: 0.944676\n",
            "[491]\tvalid_0's multi_logloss: 0.944689\n",
            "[492]\tvalid_0's multi_logloss: 0.944646\n",
            "[493]\tvalid_0's multi_logloss: 0.944593\n",
            "[494]\tvalid_0's multi_logloss: 0.944541\n",
            "[495]\tvalid_0's multi_logloss: 0.944488\n",
            "[496]\tvalid_0's multi_logloss: 0.944446\n",
            "[497]\tvalid_0's multi_logloss: 0.944361\n",
            "[498]\tvalid_0's multi_logloss: 0.944247\n",
            "[499]\tvalid_0's multi_logloss: 0.944163\n",
            "[500]\tvalid_0's multi_logloss: 0.944031\n",
            "[501]\tvalid_0's multi_logloss: 0.94401\n",
            "[502]\tvalid_0's multi_logloss: 0.943915\n",
            "[503]\tvalid_0's multi_logloss: 0.943804\n",
            "[504]\tvalid_0's multi_logloss: 0.943805\n",
            "[505]\tvalid_0's multi_logloss: 0.943823\n",
            "[506]\tvalid_0's multi_logloss: 0.943787\n",
            "[507]\tvalid_0's multi_logloss: 0.943648\n",
            "[508]\tvalid_0's multi_logloss: 0.943589\n",
            "[509]\tvalid_0's multi_logloss: 0.943451\n",
            "[510]\tvalid_0's multi_logloss: 0.943478\n",
            "[511]\tvalid_0's multi_logloss: 0.943353\n",
            "[512]\tvalid_0's multi_logloss: 0.94318\n",
            "[513]\tvalid_0's multi_logloss: 0.943056\n",
            "[514]\tvalid_0's multi_logloss: 0.942964\n",
            "[515]\tvalid_0's multi_logloss: 0.942962\n",
            "[516]\tvalid_0's multi_logloss: 0.942882\n",
            "[517]\tvalid_0's multi_logloss: 0.942924\n",
            "[518]\tvalid_0's multi_logloss: 0.942867\n",
            "[519]\tvalid_0's multi_logloss: 0.942814\n",
            "[520]\tvalid_0's multi_logloss: 0.942796\n",
            "[521]\tvalid_0's multi_logloss: 0.942634\n",
            "[522]\tvalid_0's multi_logloss: 0.942573\n",
            "[523]\tvalid_0's multi_logloss: 0.94252\n",
            "[524]\tvalid_0's multi_logloss: 0.942464\n",
            "[525]\tvalid_0's multi_logloss: 0.942456\n",
            "[526]\tvalid_0's multi_logloss: 0.94239\n",
            "[527]\tvalid_0's multi_logloss: 0.942306\n",
            "[528]\tvalid_0's multi_logloss: 0.942241\n",
            "[529]\tvalid_0's multi_logloss: 0.9422\n",
            "[530]\tvalid_0's multi_logloss: 0.942194\n",
            "[531]\tvalid_0's multi_logloss: 0.942124\n",
            "[532]\tvalid_0's multi_logloss: 0.94209\n",
            "[533]\tvalid_0's multi_logloss: 0.942007\n",
            "[534]\tvalid_0's multi_logloss: 0.941963\n",
            "[535]\tvalid_0's multi_logloss: 0.94187\n",
            "[536]\tvalid_0's multi_logloss: 0.941839\n",
            "[537]\tvalid_0's multi_logloss: 0.941922\n",
            "[538]\tvalid_0's multi_logloss: 0.941845\n",
            "[539]\tvalid_0's multi_logloss: 0.941882\n",
            "[540]\tvalid_0's multi_logloss: 0.941888\n",
            "[541]\tvalid_0's multi_logloss: 0.941917\n",
            "[542]\tvalid_0's multi_logloss: 0.941876\n",
            "[543]\tvalid_0's multi_logloss: 0.941845\n",
            "[544]\tvalid_0's multi_logloss: 0.941819\n",
            "[545]\tvalid_0's multi_logloss: 0.941826\n",
            "[546]\tvalid_0's multi_logloss: 0.941796\n",
            "[547]\tvalid_0's multi_logloss: 0.941788\n",
            "[548]\tvalid_0's multi_logloss: 0.941828\n",
            "[549]\tvalid_0's multi_logloss: 0.941843\n",
            "[550]\tvalid_0's multi_logloss: 0.941779\n",
            "[551]\tvalid_0's multi_logloss: 0.941727\n",
            "[552]\tvalid_0's multi_logloss: 0.94172\n",
            "[553]\tvalid_0's multi_logloss: 0.941721\n",
            "[554]\tvalid_0's multi_logloss: 0.941629\n",
            "[555]\tvalid_0's multi_logloss: 0.941532\n",
            "[556]\tvalid_0's multi_logloss: 0.941493\n",
            "[557]\tvalid_0's multi_logloss: 0.941438\n",
            "[558]\tvalid_0's multi_logloss: 0.941341\n",
            "[559]\tvalid_0's multi_logloss: 0.941379\n",
            "[560]\tvalid_0's multi_logloss: 0.941296\n",
            "[561]\tvalid_0's multi_logloss: 0.941318\n",
            "[562]\tvalid_0's multi_logloss: 0.94124\n",
            "[563]\tvalid_0's multi_logloss: 0.941172\n",
            "[564]\tvalid_0's multi_logloss: 0.941192\n",
            "[565]\tvalid_0's multi_logloss: 0.941116\n",
            "[566]\tvalid_0's multi_logloss: 0.941125\n",
            "[567]\tvalid_0's multi_logloss: 0.941064\n",
            "[568]\tvalid_0's multi_logloss: 0.941035\n",
            "[569]\tvalid_0's multi_logloss: 0.94111\n",
            "[570]\tvalid_0's multi_logloss: 0.94108\n",
            "[571]\tvalid_0's multi_logloss: 0.941115\n",
            "[572]\tvalid_0's multi_logloss: 0.941016\n",
            "[573]\tvalid_0's multi_logloss: 0.940957\n",
            "[574]\tvalid_0's multi_logloss: 0.940926\n",
            "[575]\tvalid_0's multi_logloss: 0.940921\n",
            "[576]\tvalid_0's multi_logloss: 0.940964\n",
            "[577]\tvalid_0's multi_logloss: 0.94092\n",
            "[578]\tvalid_0's multi_logloss: 0.940903\n",
            "[579]\tvalid_0's multi_logloss: 0.940743\n",
            "[580]\tvalid_0's multi_logloss: 0.9407\n",
            "[581]\tvalid_0's multi_logloss: 0.940716\n",
            "[582]\tvalid_0's multi_logloss: 0.940619\n",
            "[583]\tvalid_0's multi_logloss: 0.940582\n",
            "[584]\tvalid_0's multi_logloss: 0.940704\n",
            "[585]\tvalid_0's multi_logloss: 0.940637\n",
            "[586]\tvalid_0's multi_logloss: 0.940658\n",
            "[587]\tvalid_0's multi_logloss: 0.940689\n",
            "[588]\tvalid_0's multi_logloss: 0.940674\n",
            "[589]\tvalid_0's multi_logloss: 0.940731\n",
            "[590]\tvalid_0's multi_logloss: 0.94073\n",
            "[591]\tvalid_0's multi_logloss: 0.940715\n",
            "[592]\tvalid_0's multi_logloss: 0.940647\n",
            "[593]\tvalid_0's multi_logloss: 0.940564\n",
            "[594]\tvalid_0's multi_logloss: 0.940586\n",
            "[595]\tvalid_0's multi_logloss: 0.940528\n",
            "[596]\tvalid_0's multi_logloss: 0.940505\n",
            "[597]\tvalid_0's multi_logloss: 0.940429\n",
            "[598]\tvalid_0's multi_logloss: 0.940448\n",
            "[599]\tvalid_0's multi_logloss: 0.940351\n",
            "[600]\tvalid_0's multi_logloss: 0.940308\n",
            "[601]\tvalid_0's multi_logloss: 0.940288\n",
            "[602]\tvalid_0's multi_logloss: 0.940218\n",
            "[603]\tvalid_0's multi_logloss: 0.940184\n",
            "[604]\tvalid_0's multi_logloss: 0.940154\n",
            "[605]\tvalid_0's multi_logloss: 0.940108\n",
            "[606]\tvalid_0's multi_logloss: 0.940047\n",
            "[607]\tvalid_0's multi_logloss: 0.940079\n",
            "[608]\tvalid_0's multi_logloss: 0.94001\n",
            "[609]\tvalid_0's multi_logloss: 0.939951\n",
            "[610]\tvalid_0's multi_logloss: 0.939893\n",
            "[611]\tvalid_0's multi_logloss: 0.939851\n",
            "[612]\tvalid_0's multi_logloss: 0.939873\n",
            "[613]\tvalid_0's multi_logloss: 0.939914\n",
            "[614]\tvalid_0's multi_logloss: 0.939847\n",
            "[615]\tvalid_0's multi_logloss: 0.939712\n",
            "[616]\tvalid_0's multi_logloss: 0.93971\n",
            "[617]\tvalid_0's multi_logloss: 0.939638\n",
            "[618]\tvalid_0's multi_logloss: 0.939657\n",
            "[619]\tvalid_0's multi_logloss: 0.939667\n",
            "[620]\tvalid_0's multi_logloss: 0.93968\n",
            "[621]\tvalid_0's multi_logloss: 0.939651\n",
            "[622]\tvalid_0's multi_logloss: 0.939651\n",
            "[623]\tvalid_0's multi_logloss: 0.939664\n",
            "[624]\tvalid_0's multi_logloss: 0.939603\n",
            "[625]\tvalid_0's multi_logloss: 0.939543\n",
            "[626]\tvalid_0's multi_logloss: 0.939474\n",
            "[627]\tvalid_0's multi_logloss: 0.939497\n",
            "[628]\tvalid_0's multi_logloss: 0.939491\n",
            "[629]\tvalid_0's multi_logloss: 0.939487\n",
            "[630]\tvalid_0's multi_logloss: 0.939406\n",
            "[631]\tvalid_0's multi_logloss: 0.939446\n",
            "[632]\tvalid_0's multi_logloss: 0.939481\n",
            "[633]\tvalid_0's multi_logloss: 0.939526\n",
            "[634]\tvalid_0's multi_logloss: 0.939524\n",
            "[635]\tvalid_0's multi_logloss: 0.939461\n",
            "[636]\tvalid_0's multi_logloss: 0.939438\n",
            "[637]\tvalid_0's multi_logloss: 0.939453\n",
            "[638]\tvalid_0's multi_logloss: 0.939364\n",
            "[639]\tvalid_0's multi_logloss: 0.939377\n",
            "[640]\tvalid_0's multi_logloss: 0.939408\n",
            "[641]\tvalid_0's multi_logloss: 0.939393\n",
            "[642]\tvalid_0's multi_logloss: 0.939362\n",
            "[643]\tvalid_0's multi_logloss: 0.939323\n",
            "[644]\tvalid_0's multi_logloss: 0.939314\n",
            "[645]\tvalid_0's multi_logloss: 0.93927\n",
            "[646]\tvalid_0's multi_logloss: 0.939308\n",
            "[647]\tvalid_0's multi_logloss: 0.93924\n",
            "[648]\tvalid_0's multi_logloss: 0.939241\n",
            "[649]\tvalid_0's multi_logloss: 0.939222\n",
            "[650]\tvalid_0's multi_logloss: 0.939218\n",
            "[651]\tvalid_0's multi_logloss: 0.939202\n",
            "[652]\tvalid_0's multi_logloss: 0.939067\n",
            "[653]\tvalid_0's multi_logloss: 0.939048\n",
            "[654]\tvalid_0's multi_logloss: 0.938997\n",
            "[655]\tvalid_0's multi_logloss: 0.938911\n",
            "[656]\tvalid_0's multi_logloss: 0.938804\n",
            "[657]\tvalid_0's multi_logloss: 0.938737\n",
            "[658]\tvalid_0's multi_logloss: 0.938719\n",
            "[659]\tvalid_0's multi_logloss: 0.93866\n",
            "[660]\tvalid_0's multi_logloss: 0.938586\n",
            "[661]\tvalid_0's multi_logloss: 0.93858\n",
            "[662]\tvalid_0's multi_logloss: 0.938558\n",
            "[663]\tvalid_0's multi_logloss: 0.938467\n",
            "[664]\tvalid_0's multi_logloss: 0.938532\n",
            "[665]\tvalid_0's multi_logloss: 0.938549\n",
            "[666]\tvalid_0's multi_logloss: 0.938462\n",
            "[667]\tvalid_0's multi_logloss: 0.938419\n",
            "[668]\tvalid_0's multi_logloss: 0.93837\n",
            "[669]\tvalid_0's multi_logloss: 0.938319\n",
            "[670]\tvalid_0's multi_logloss: 0.938316\n",
            "[671]\tvalid_0's multi_logloss: 0.938254\n",
            "[672]\tvalid_0's multi_logloss: 0.938238\n",
            "[673]\tvalid_0's multi_logloss: 0.938135\n",
            "[674]\tvalid_0's multi_logloss: 0.938098\n",
            "[675]\tvalid_0's multi_logloss: 0.938058\n",
            "[676]\tvalid_0's multi_logloss: 0.938046\n",
            "[677]\tvalid_0's multi_logloss: 0.938055\n",
            "[678]\tvalid_0's multi_logloss: 0.938102\n",
            "[679]\tvalid_0's multi_logloss: 0.93803\n",
            "[680]\tvalid_0's multi_logloss: 0.937983\n",
            "[681]\tvalid_0's multi_logloss: 0.937941\n",
            "[682]\tvalid_0's multi_logloss: 0.937922\n",
            "[683]\tvalid_0's multi_logloss: 0.937966\n",
            "[684]\tvalid_0's multi_logloss: 0.937965\n",
            "[685]\tvalid_0's multi_logloss: 0.937964\n",
            "[686]\tvalid_0's multi_logloss: 0.937998\n",
            "[687]\tvalid_0's multi_logloss: 0.93801\n",
            "[688]\tvalid_0's multi_logloss: 0.93806\n",
            "[689]\tvalid_0's multi_logloss: 0.938006\n",
            "[690]\tvalid_0's multi_logloss: 0.93794\n",
            "[691]\tvalid_0's multi_logloss: 0.937862\n",
            "[692]\tvalid_0's multi_logloss: 0.93797\n",
            "[693]\tvalid_0's multi_logloss: 0.937946\n",
            "[694]\tvalid_0's multi_logloss: 0.937948\n",
            "[695]\tvalid_0's multi_logloss: 0.937838\n",
            "[696]\tvalid_0's multi_logloss: 0.937866\n",
            "[697]\tvalid_0's multi_logloss: 0.937867\n",
            "[698]\tvalid_0's multi_logloss: 0.937783\n",
            "[699]\tvalid_0's multi_logloss: 0.937784\n",
            "[700]\tvalid_0's multi_logloss: 0.937745\n",
            "[701]\tvalid_0's multi_logloss: 0.937718\n",
            "[702]\tvalid_0's multi_logloss: 0.937639\n",
            "[703]\tvalid_0's multi_logloss: 0.937621\n",
            "[704]\tvalid_0's multi_logloss: 0.937671\n",
            "[705]\tvalid_0's multi_logloss: 0.937572\n",
            "[706]\tvalid_0's multi_logloss: 0.937531\n",
            "[707]\tvalid_0's multi_logloss: 0.937462\n",
            "[708]\tvalid_0's multi_logloss: 0.937457\n",
            "[709]\tvalid_0's multi_logloss: 0.937415\n",
            "[710]\tvalid_0's multi_logloss: 0.937331\n",
            "[711]\tvalid_0's multi_logloss: 0.937281\n",
            "[712]\tvalid_0's multi_logloss: 0.937265\n",
            "[713]\tvalid_0's multi_logloss: 0.937347\n",
            "[714]\tvalid_0's multi_logloss: 0.937373\n",
            "[715]\tvalid_0's multi_logloss: 0.937339\n",
            "[716]\tvalid_0's multi_logloss: 0.937332\n",
            "[717]\tvalid_0's multi_logloss: 0.937299\n",
            "[718]\tvalid_0's multi_logloss: 0.93735\n",
            "[719]\tvalid_0's multi_logloss: 0.93736\n",
            "[720]\tvalid_0's multi_logloss: 0.937358\n",
            "[721]\tvalid_0's multi_logloss: 0.937399\n",
            "[722]\tvalid_0's multi_logloss: 0.937344\n",
            "Early stopping, best iteration is:\n",
            "[712]\tvalid_0's multi_logloss: 0.937265\n",
            "Train Accuracy: 0.9590098482832047\n",
            "Validation Accuracy: 0.5745341614906833\n",
            "Test Accuracy: 0.5611421477343265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uw3fcMHMB4S-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}